{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "import glob\n",
    "import os.path as osp\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models=[\"resnet18\", \"resnet34\", \"xception\"]\n",
    "all_datasets=['cifar10']\n",
    "all_optim = ['adam', 'sgd', 'rmsprobe', 'sparseadam']\n",
    "all_initialization = ['pretrain', 'kaiming_normal']\n",
    "all_lr_scheduler = ['reduceLR', 'none', 'cosine_annealingLR']\n",
    "all_noise_injection = [\"0.0\", \"0.03\", \"0.07\", \"0.13\"]\n",
    "all_noise_sparsity = [\"0.0\", \"0.2\", \"0.4\", \"0.6\"]\n",
    "all_lr_rate = ['0.001', '0.1']\n",
    "all_folds = ['0', '1', '2']\n",
    "all_phase = ['train', 'validation']\n",
    "all_epochs = 5\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the index of the maximum value in a list\n",
    "def argmax_list(lst):\n",
    "    return max(range(len(lst)), key=lst.__getitem__)\n",
    "# Define a function to calculate entropy\n",
    "def entropy(probabilities):\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_density_2D(path=\"img.png\", dataframes=[], target_cols=[], cmaps=[], labels=[], title=\"\", alphas=[]):\n",
    "    plt.figure()  # Create a new figure for each plot\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    for i, df in enumerate(dataframes):\n",
    "        if len(df):\n",
    "            sns.kdeplot(x=df[target_cols[0]], y=df[target_cols[1]], cmap=cmaps[i], fill=True, bw_adjust=0.5, alpha=alphas[i])\n",
    "    # plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.savefig(path)\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "root = osp.join(\"/home/vision/Repo/cleanset/logs\")\n",
    "iterations_log = None\n",
    "for dataset_name in all_datasets:\n",
    "    dataset_info = pd.read_csv(f\"dataset/{dataset_name}/info.csv\")\n",
    "    dataset_info[dataset_info['phase'] == 'train']\n",
    "    for model_name in all_models:\n",
    "        for optim in all_optim:\n",
    "            for initialization in all_initialization:\n",
    "                for lr_scheduler in all_lr_scheduler:\n",
    "                    for noise_injection in all_noise_injection:\n",
    "                        for noise_sparsity in all_noise_sparsity:\n",
    "                            for lr_rate in all_lr_rate:\n",
    "                                columns = {'epoch': int, 'fold': int, 'sample': float, 'label': float, 'phase': float, 'entropy': str}\n",
    "                                samples_training_data = pd.DataFrame(columns=columns.keys())\n",
    "                                for phase in all_phase:\n",
    "                                    for epoch in range(all_epochs):\n",
    "                                        all_fold_loss = []\n",
    "                                        epoch = f\"{epoch :03d}\"\n",
    "                                        for fold in all_folds:\n",
    "                                            glob_regex = osp.join(root,dataset_name, model_name, optim, initialization, lr_scheduler, f\"np={noise_injection}\", f\"ns={noise_sparsity}\", f\"lr={lr_rate}\", fold, phase, epoch, '*.pd')\n",
    "                                            iterations_log = sorted(glob.glob(glob_regex))\n",
    "                                            if len(iterations_log) == 0:\n",
    "                                                continue\n",
    "                                            # print(\"iterations_log: \", iterations_log)\n",
    "                                            iterations_log = [pd.read_pickle(file_path) for file_path in iterations_log]\n",
    "                                            iterations_log = pd.concat(iterations_log, axis=0, ignore_index=True)\n",
    "\n",
    "                                            iterations_log = iterations_log.drop(columns=['loss'])\n",
    "                                            iterations_log['entropy'] = iterations_log['proba'].apply(lambda x: entropy(x))\n",
    "                                            iterations_log = iterations_log.drop(columns=['proba'])\n",
    "\n",
    "                                            iterations_log['phase'] = phase\n",
    "                                            iterations_log['fold'] = fold\n",
    "                                            iterations_log['epoch'] = epoch\n",
    "                                            samples_training_data = samples_training_data._append(iterations_log)\n",
    "                                if len(samples_training_data):\n",
    "                                    entropy_stats = samples_training_data.groupby(['sample', 'label'])['entropy'].agg(['mean', 'std']).reset_index()\n",
    "                                    entropy_stats['mean'] = scaler.fit_transform(entropy_stats[['mean']])\n",
    "                                    entropy_stats['std'] = scaler.fit_transform(entropy_stats[['std']])\n",
    "                                    merged_df = pd.merge(entropy_stats, dataset_info[['index', 'true_label']], left_on='sample', right_on='index', how='inner')\n",
    "                                    true_label_sample_entropy_stats = merged_df[merged_df['label'] == merged_df['true_label']]\n",
    "                                    wrong_label_sample_entropy_stats = merged_df[merged_df['label'] != merged_df['true_label']]\n",
    "\n",
    "                                    plot_density_2D(\n",
    "                                        path=osp.join(root, dataset_name, model_name, optim, initialization, lr_scheduler, f\"np={noise_injection}\", f\"ns={noise_sparsity}\", f\"lr={lr_rate}\", \"entropy.png\"),\n",
    "                                        dataframes=[true_label_sample_entropy_stats, wrong_label_sample_entropy_stats],\n",
    "                                        target_cols=['mean', 'std'], cmaps=['Greens', 'Reds'], alphas=[1, 0.5],\n",
    "                                        labels=['true labels', 'wrong labels'],\n",
    "                                        title=f\"Entropy stats || {dataset_name} {model_name} {optim} {initialization} {lr_scheduler} | np={noise_injection} ns={noise_sparsity}\")\n",
    "                                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
