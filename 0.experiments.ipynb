{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os.path as osp\n",
    "import os\n",
    "from configs.general import \\\n",
    "                    MODELS, DATASETS, OPTIMS,\\\n",
    "                    INITIALIZATIONS, LR_SCHEDULERS, NPS, NSS,\\\n",
    "                    PHASES, EPOCHS, FOLDS, DEVICE,\\\n",
    "                    DATA_FILTERING_POLICIES, DATA_RETRIEVAL_POLICIES, \\\n",
    "                    EXPERIMENT_COLS, EXPERIMENT_BASE_DIR, EXPERIMENT_INFO_PATH,\\\n",
    "                    FILTERING_EXPERIMENT_INFO_PATH, FILTERING_EXPERIMENT_BASE_DIR, FILTERING_EXPERIMENT_COLS, DROPOUTS, TRANSFORM_LEVELS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create base experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame()\n",
    "for dataset in DATASETS:\n",
    "    for model in MODELS:\n",
    "        for optim, LRS in OPTIMS:\n",
    "            for lr in LRS:\n",
    "                for lr_ch in LR_SCHEDULERS:\n",
    "                    for init in INITIALIZATIONS:\n",
    "                        for _np in NPS:\n",
    "                            for ns in NSS:\n",
    "                                if _np == \"0.0\" and ns != \"0.0\":\n",
    "                                    continue\n",
    "                                for dropout in DROPOUTS:\n",
    "                                    for transform in TRANSFORM_LEVELS:\n",
    "                                        row = {\n",
    "                                            \"dataset\": dataset,\n",
    "                                            \"model\": model,\n",
    "                                            \"dropout\": f\"drp={dropout}\",\n",
    "                                            \"optim\": optim,\n",
    "                                            \"lr\": f\"lr={lr}\",\n",
    "                                            \"lr_scheduler\": lr_ch,\n",
    "                                            \"init\": init,\n",
    "                                            \"transform\": transform,\n",
    "                                            \"np\": f\"np={_np}\",\n",
    "                                            \"ns\": f\"ns={ns}\",\n",
    "                                        }\n",
    "                                        table = table._append(row, ignore_index=True)\n",
    "\n",
    "table['folds'] = FOLDS\n",
    "table['epochs'] = EPOCHS\n",
    "table['done'] = False\n",
    "table['valid'] = False\n",
    "table['test_acc'] = None\n",
    "table['train_loss'] = None\n",
    "table['validation_loss'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3744"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def hash_row_data(row):\n",
    "    # Convert row data to a string and encode it\n",
    "    row_str = str(row)\n",
    "    row_bytes = row_str.encode('utf-8')\n",
    "    # Apply SHA-256 hash function\n",
    "    hashed_data = hashlib.sha256(row_bytes).hexdigest()[:10]\n",
    "    return hashed_data\n",
    "\n",
    "table['index'] = table[EXPERIMENT_COLS].apply(hash_row_data, axis=1)\n",
    "table.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optim\n",
       "adam    [lr=0.001, lr=0.0001]\n",
       "sgd         [lr=0.1, lr=0.01]\n",
       "Name: lr, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = table.groupby('optim')['lr'].agg(pd.Series.unique)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np\n",
       "np=0.0                             [ns=0.0]\n",
       "np=0.03    [ns=0.0, ns=0.2, ns=0.4, ns=0.6]\n",
       "np=0.07    [ns=0.0, ns=0.2, ns=0.4, ns=0.6]\n",
       "np=0.13    [ns=0.0, ns=0.2, ns=0.4, ns=0.6]\n",
       "Name: ns, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = table.groupby('np')['ns'].agg(pd.Series.unique)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: dataset: ['cifar10' 'cifar100' 'mnist']\n",
      "Column: model: ['resnet18' 'resnet34' 'xception']\n",
      "Column: dropout: ['drp=0' 'drp=0.3']\n",
      "Column: optim: ['adam' 'sgd']\n",
      "Column: lr: ['lr=0.001' 'lr=0.0001' 'lr=0.1' 'lr=0.01']\n",
      "Column: lr_scheduler: ['none']\n",
      "Column: init: ['pretrain' 'kaiming_normal']\n",
      "Column: transform: ['default' 'intermediate']\n",
      "Column: np: ['np=0.0' 'np=0.03' 'np=0.07' 'np=0.13']\n",
      "Column: ns: ['ns=0.0' 'ns=0.2' 'ns=0.4' 'ns=0.6']\n",
      "Column: folds: [3]\n",
      "Column: epochs: [15]\n",
      "Column: done: [False]\n",
      "Column: valid: [False]\n",
      "Column: test_acc: [None]\n",
      "Column: train_loss: [None]\n",
      "Column: validation_loss: [None]\n",
      "3744\n"
     ]
    }
   ],
   "source": [
    "for column in table.columns:\n",
    "    unique_values = table[column].unique()\n",
    "    print(f\"Column: {column}: {unique_values}\")\n",
    "print(len(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table.to_csv('experiments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update experiments status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_experiments = pd.read_csv(EXPERIMENT_INFO_PATH, index_col='index')\n",
    "experiments = old_experiments.copy()\n",
    "for index, row in experiments.iterrows():\n",
    "    experiment_dir = osp.join(EXPERIMENT_BASE_DIR, *[str(row[col]) for col in EXPERIMENT_COLS])\n",
    "    if osp.isdir(experiment_dir):\n",
    "        number_of_folds = len(os.listdir(experiment_dir))\n",
    "        if number_of_folds == FOLDS:\n",
    "            experiments.loc[index, 'done'] = True\n",
    "\n",
    "print(\"total experiments:\", len(old_experiments))\n",
    "print(\"already done:\", (old_experiments['done'] == True).sum())\n",
    "print(\"required to update:\", (experiments['done'] == True).sum() -(old_experiments['done'] == True).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.to_csv(EXPERIMENT_INFO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update filtering experiments status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_experiments = pd.read_csv(FILTERING_EXPERIMENT_INFO_PATH, index_col='index')\n",
    "experiments = old_experiments.copy()\n",
    "for index, row in experiments.iterrows():\n",
    "    experiment_dir = osp.join(FILTERING_EXPERIMENT_BASE_DIR, *[str(row[col]) for col in FILTERING_EXPERIMENT_COLS])\n",
    "    if osp.isdir(experiment_dir):\n",
    "        number_of_folds = len(os.listdir(experiment_dir))\n",
    "        if number_of_folds == FOLDS:\n",
    "            experiments.loc[index, 'done'] = True\n",
    "\n",
    "print(\"total experiments:\", len(old_experiments))\n",
    "print(\"already done:\", (old_experiments['done'] == True).sum())\n",
    "print(\"required to update:\", (experiments['done'] == True).sum() -(old_experiments['done'] == True).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.to_csv(FILTERING_EXPERIMENT_INFO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put last vall loss and train loss in dataframe for done experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = pd.read_csv(EXPERIMENT_INFO_PATH, index_col='index')\n",
    "experiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_experiments = experiments[(experiments['done'] == True) & (experiments['validation_loss'].isna() | experiments['train_loss'].isna())]\n",
    "print('number of experiments: ', len(target_experiments))\n",
    "target_experiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in target_experiments.iterrows():\n",
    "    experiment_dir = osp.join(EXPERIMENT_BASE_DIR, *[str(row[col]) for col in EXPERIMENT_COLS])\n",
    "    if osp.isdir(experiment_dir):\n",
    "        number_of_folds = len(os.listdir(experiment_dir))\n",
    "        if number_of_folds == FOLDS:\n",
    "            lossie = {'train': [], 'validation': []}\n",
    "            for fold in range(FOLDS):\n",
    "                fold_experiment_dir = osp.join(experiment_dir, str(fold))\n",
    "                samples_data = pd.DataFrame()\n",
    "                for phase in PHASES:\n",
    "                    phase_experiment_dir = osp.join(fold_experiment_dir, phase)\n",
    "                    last_epoch_experiment_dir = osp.join(phase_experiment_dir, f\"{EPOCHS - 1 :03d}\")\n",
    "                    glob_regex = osp.join(last_epoch_experiment_dir, '*.pd')\n",
    "                    iterations_log = sorted(glob.glob(glob_regex))\n",
    "                    if len(iterations_log) == 0:\n",
    "                        print(f\"Train for this experiment is not complete \\n{row}\")\n",
    "                        continue\n",
    "                    iterations_log = [pd.read_pickle(file_path) for file_path in iterations_log]\n",
    "                    iterations_log = pd.concat(iterations_log, axis=0, ignore_index=True)\n",
    "                    iterations_log = iterations_log.drop(columns=['proba'])\n",
    "                    samples_data = samples_data._append(iterations_log, ignore_index=True)\n",
    "                    mean_loss = round(float(samples_data['loss'].mean()), 3)\n",
    "                    lossie[phase].append(mean_loss)\n",
    "            print(lossie)\n",
    "\n",
    "            experiments.loc[index, 'validation_loss'] = f\"{lossie['validation'][0]} {lossie['validation'][1]} {lossie['validation'][2]}\"\n",
    "            experiments.loc[index, 'train_loss'] = f\"{lossie['train'][0]} {lossie['train'][1]} {lossie['train'][2]}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments[(experiments['done'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.to_csv(EXPERIMENT_INFO_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
